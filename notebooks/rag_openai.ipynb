{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import openai\n",
    "from pinecone import Pinecone\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/shuai/Desktop/clear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557284\n"
     ]
    }
   ],
   "source": [
    "formatted_series = df.apply(lambda x: f\"{x['title']}: {x['lyrics']}\", axis=1)\n",
    "\n",
    "# Combine all formatted strings into a single string, separated by new lines\n",
    "data = '\\n'.join(formatted_series)\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text):\n",
    "    \"\"\"\n",
    "    Function to get the chunks of text from the raw text\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text from the PDF file\n",
    "\n",
    "    Returns:\n",
    "        chunks (list): The list of chunks of text\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the text splitter\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\", # Split the text by new line\n",
    "        chunk_size=1250, # Split the text into chunks of 1600 characters\n",
    "        chunk_overlap=200, # Overlap the chunks by 200 characters\n",
    "        length_function=len # Use the length function to get the length of the text\n",
    "    )\n",
    "\n",
    "    # Get the chunks of text\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunk_data = get_chunks(data)\n",
    "\n",
    "\n",
    "def get_embeddings(chunk_data):\n",
    "    \"\"\"\n",
    "    Get the embedding vectors for the chunk data\n",
    "\n",
    "    Arg:\n",
    "    - chunk_data: a list of chunk data\n",
    "\n",
    "    Return:\n",
    "    - Embedded vectors\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    client = openai.OpenAI(api_key = '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')\n",
    "    response = client.embeddings.create(\n",
    "        input=chunk_data,\n",
    "        model=\"text-embedding-3-small\"\n",
    "        )\n",
    "\n",
    "    vectors_list = [item.embedding for item in response.data]\n",
    "    return vectors_list\n",
    "\n",
    "vectors_list = get_embeddings(chunk_data)\n",
    "\n",
    "pc = Pinecone(api_key='6e30cd30-b1fb-4fd9-849f-1507d42c1df2')\n",
    "index = pc.Index(\"openai153\")\n",
    "\n",
    "# Store vectors in vector database\n",
    "def vector_store(vectors_list):\n",
    "    # Iterate over the vectors_list\n",
    "    for i in range(len(vectors_list)):\n",
    "        index.upsert(\n",
    "            vectors=[\n",
    "                {\n",
    "                    'id': f'vec_{i}',\n",
    "                    'values': vectors_list[i],\n",
    "                    'metadata': {\"text\":chunk_data[i]}\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "vector_store(vectors_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     chunk_data = get_chunks(data)\n",
    "#     vectors_list = get_embeddings(chunk_data)\n",
    "#     pc = Pinecone(api_key='7658c7bf-1ee9-4e8d-9a5e-bc100843b51f')\n",
    "#     index = pc.Index(\"openai\")\n",
    "#     vector_store(vectors_list)\n",
    "#     context = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'create a poem for me in the style of Bob Dylan in this topic: Rivers of Time'\n",
    "query_embedding = client.embeddings.create(\n",
    "    input=query,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "query_vector = [item.embedding for item in query_embedding.data]\n",
    "\n",
    "def retrieve_embedding(index, num_embed):\n",
    "    \"\"\"\n",
    "    Convert the information of vectors in the database into a panda dataframe\n",
    "    \n",
    "    Args:\n",
    "    - index: Name of vector database(already set up)\n",
    "    - num_embed: total number of vectors in the vector databse\n",
    "\n",
    "    Return:\n",
    "    - a dataframe which contains the embedded vectors and corresponding text\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store embedding data\n",
    "    embedding_data = {\"id\":[], \"values\":[], \"text\":[]}\n",
    "    \n",
    "    # Fetch the embeddings \n",
    "    embedding = index.fetch([f'vec_{i}' for i in range(num_embed)])\n",
    "    \n",
    "    for i in range(num_embed):\n",
    "        embedding_data[\"id\"].append(i)\n",
    "        idx = f\"vec_{i}\"\n",
    "        embedding_data[\"text\"].append(embedding['vectors'][idx]['metadata']['text'])\n",
    "        embedding_data[\"values\"].append(embedding['vectors'][idx]['values'])\n",
    "        \n",
    "    return pd.DataFrame(embedding_data)\n",
    "\n",
    "\n",
    "embedding_data = retrieve_embedding(index,len(vectors_list))\n",
    "\n",
    "def semantic_search(query_vector, db_embeddings):\n",
    "    \"\"\"\n",
    "    Find the top three vectors which have the highest comsine similarity with the query vector\n",
    "\n",
    "    Args:\n",
    "    - query_vector: embedded vector of user query\n",
    "    - db_embeddings: embedded vectors from vector database\n",
    "\n",
    "    Return:\n",
    "    - The indices of top three most similar vectors with the query vector\n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, db_embeddings)[0]\n",
    "    # Get the indices of the top three similarity scores\n",
    "    top_10_indices = np.argsort(similarities)[-10:][::-1]  # This sorts and then reverses to get top 3\n",
    "    # Retrieve the top three most similar chunks and their similarity scores\n",
    "    \n",
    "    return top_10_indices\n",
    "\n",
    "top_10_indices = semantic_search(query_vector, vectors_list)\n",
    "\n",
    "\n",
    "def get_text(embedding_data, top_10_indices):\n",
    "    \"\"\"\n",
    "    Extracts text corresponding to the given top vectors from embedding data.\n",
    "\n",
    "    Args:\n",
    "    - embedding_data (DataFrame): DataFrame containing columns 'id', 'values', and 'text'.\n",
    "    - top_vectors (list): List of indices for which corresponding text needs to be extracted.\n",
    "\n",
    "    Returns:\n",
    "    - combined_text (str): Combined text corresponding to the top vectors.\n",
    "    \"\"\"\n",
    "   # Extract text from selected rows\n",
    "    selected_texts = embedding_data.loc[top_10_indices, 'text'].tolist()\n",
    "\n",
    "    # Combine the selected texts into a single string\n",
    "    combined_text = ' '.join(selected_texts)\n",
    "\n",
    "    return combined_text\n",
    "\n",
    "context = get_text(embedding_data, top_10_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Down the rivers of time, we sail and we roam\\nThrough the twisting waters, we find our way home\\nMemories like echoes, linger in the air\\nWhispering secrets of joys and despair\\n\\nThe current keeps moving, never looking back\\nCarrying us forward on an endless track\\nThrough valleys of laughter and mountains of tears\\nWe navigate bravely, overcoming our fears\\n\\nEach bend brings a story, a lesson to find\\nIn the rivers of time, where the past intertwines\\nWith the present unfolding, a mystery untold\\nIn the flow of existence, where destinies mold\\n\\nSo let's navigate softly, with hearts open wide\\nEmbrace every moment, let love be our guide\\nFor in the rivers of time, where we ebb and we flow\\nWe discover the beauty of life's sacred glow\\n\\nSo sail on, my friend, in the rivers of time\\nEmbrace every twist and turn, and make each moment shine\\nFor in the depths of the waters, where the soul finds its rhyme\\nWe dance with the rhythm of love's infinite chime\""
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = openai.OpenAI(api_key = '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')\n",
    "def response_rag(user_input):\n",
    "    \"\"\"\n",
    "    Generate response using rag\n",
    "\n",
    "    Args:\n",
    "    - questions given by users\n",
    "\n",
    "    Returns:\n",
    "    - answer using rag  \n",
    "    \"\"\"\n",
    "    \n",
    "    response_rag = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"assistant\", \"content\": context},\n",
    "      {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    )\n",
    "    return response_rag.choices[0].message.content\n",
    "\n",
    "response_rag('create a poem for me in the style of Bob Dylan in this topic: Rivers of Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the rivers of time, we all do flow\\nUpon its currents, we come and we go\\nA never-ending journey, twisting and turning\\nAs the days pass by, the flames keep burning\\n\\nFrom the cradle to the grave, we journey on\\nThrough the trials and tribulations we're drawn\\nThe rivers of time, they shape and define\\nEach moment cherished, each memory sublime\\n\\nAs we walk this road, we learn and we grow\\nThrough the highs and lows, the joys and woe\\nThe rivers of time, they carry us far\\nGuiding us gently like a guiding star\\n\\nSo let us embrace the journey ahead\\nWith open hearts and minds, let us tread\\nThe rivers of time will always flow\\nBut in its depths, our stories will glow\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def response_wo_rag(user_input):\n",
    "    \"\"\"\n",
    "    Generate response withou using rag\n",
    "\n",
    "    Args:\n",
    "    - questions given by users\n",
    "\n",
    "    Returns:\n",
    "    - answer without using rag  \n",
    "    \"\"\"\n",
    "\n",
    "    response_wo_rag = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    )\n",
    "    return response_wo_rag.choices[0].message.content\n",
    "\n",
    "response_wo_rag('create a poem for me in the style of Bob Dylan in this topic: Rivers of Time')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
